{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "930690b0",
      "metadata": {},
      "source": [
        "## Setting Up the Environment\n",
        "\n",
        "We begin by importing necessary libraries:\n",
        "\n",
        "- **os, json, pathlib.Path**: For file system operations and reading configuration files.\n",
        "- **pandas**: To handle data in structured formats.\n",
        "- **transformers**: To load and work with a language model from Hugging Face.\n",
        "- **IPython.display**: To display output in Markdown format for readability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4c83ab6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import toml\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        ")\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e32acf55",
      "metadata": {},
      "source": [
        "\n",
        "This step ensures the environment has all the required tools and libraries loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a21edd9",
      "metadata": {},
      "source": [
        "## Configuration and Credentials\n",
        "\n",
        "Next, we define and set up key file paths such as the root directory, data directory, and configuration directory. The script reads Hugging Face credentials from a `credentials.json` file. This allows secure access to private model resources. We also check if the environment variable `HUGGINGFACE_TOKEN` exists to ensure authentication with Hugging Face is configured properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6495ffb7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variable HUGGINGFACE_TOKEN set.\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Paths\n",
        "paths = {\n",
        "    'root': Path.cwd().parent,\n",
        "    'data': Path.cwd().parent / \"data\",\n",
        "    \"config\": Path.cwd().parent / \"config\"\n",
        "}\n",
        "\n",
        "# Load Hugging Face credentials\n",
        "with open(paths[\"config\"] / 'credentials.json') as f:\n",
        "    credentials = json.load(f)\n",
        "\n",
        "if \"HUGGINGFACE_TOKEN\" in os.environ or \"HUGGINGFACE_TOKEN\" in credentials:\n",
        "    print(\"Environment variable HUGGINGFACE_TOKEN set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c3c333f",
      "metadata": {},
      "source": [
        "## Load External Text Prompts\n",
        "Load system description and user prompt template from external files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "205e415e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load system description and user prompt template from files\n",
        "with open(paths[\"config\"] / 'role_system_description.md', 'r') as f:\n",
        "    role_system_description = f.read()\n",
        "\n",
        "with open(paths[\"config\"] / 'user_prompt_template.md', 'r') as f:\n",
        "    user_prompt_template = f.read()\n",
        "\n",
        "with open(paths[\"config\"] / 'response_format.md', 'r') as f:\n",
        "    response_format = f.read()\n",
        "\n",
        "with open(paths[\"config\"] / 'search_criteria.md', 'r') as f:\n",
        "    search_criteria = f.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c348ddb9",
      "metadata": {},
      "source": [
        "## Loading Instructions & Candidate Data\n",
        "\n",
        "This section attempts to load candidate data from a Parquet file. If reading the Parquet file fails (e.g., due to format issues or missing file), it falls back to loading a CSV version. The data contains job titles and a ranking index, which is used for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5077ad60",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instructions successfully read!\n"
          ]
        }
      ],
      "source": [
        "# Load instructions\n",
        "try:\n",
        "    with open(paths[\"config\"] / \"instructions.md\", 'r') as file:\n",
        "        instructions = file.read()\n",
        "        print(\"Instructions successfully read!\")\n",
        "except FileNotFoundError:\n",
        "    instructions = \"\"\n",
        "    print(\"Instructions file not found.\")\n",
        "\n",
        "# Load shortlisted candidates from parquet or CSV\n",
        "try:\n",
        "    list_of_candidates = (\n",
        "        pd.read_parquet(paths['data'] / \"processed/filtered.parquet\", columns=['job_title'])['job_title']\n",
        "          .to_list()\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load parquet file: {e}. Loading CSV instead.\")\n",
        "    list_of_candidates = (\n",
        "        pd.read_csv(paths['data'] / \"processed/filtered.csv\", index_col=['rank'], usecols=['rank', 'job_title'])\n",
        "          ['job_title']\n",
        "          .to_list()\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a265859c",
      "metadata": {},
      "source": [
        "## Initializing the Language Model\n",
        "\n",
        "Here, we initialize a language model pipeline for text generation:\n",
        "\n",
        "1. **Select a Model**: We use Microsoft's Phi-3 model, a compact variant suitable for inference.\n",
        "2. **Check Device**: The script checks if a GPU is available and uses it for faster computation; otherwise, it falls back to CPU.\n",
        "3. **Load Model and Tokenizer**: The model and its tokenizer are loaded, and a text generation pipeline is set up for ease of use.\n",
        "\n",
        "This setup allows us to generate human-like text based on specified prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "86392428",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [01:24<00:00, 42.25s/it]\n",
            "Device set to use cuda\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Initialize the model and tokenizer\n",
        "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
        "\n",
        "# Adjust model loading for GPU\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    # torch_dtype=torch.float16,  # Use half precision\n",
        "    trust_remote_code=True,\n",
        "    device_map=device,  # Use GPU if available\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set up the pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c635e078",
      "metadata": {},
      "source": [
        "## Prepare Dynamic Prompt Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "52501d62",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define dynamic parameters for the prompt\n",
        "search_terms_path = paths[\"config\"] / 'search_terms.toml'\n",
        "search_terms = toml.load(search_terms_path)\n",
        "\n",
        "list_similar_roles = ', '.join(search_terms['search_phrases'])\n",
        "location = search_terms['location']\n",
        "\n",
        "# Format the user prompt using the template and parameters\n",
        "user_prompt = user_prompt_template.format(\n",
        "    location=location,\n",
        "    list_similar_roles=list_similar_roles,\n",
        "    search_criteria=search_criteria,\n",
        "    list_of_candidates=\"\\n\".join(list_of_candidates)  # Joining list elements for display\n",
        ")\n",
        "\n",
        "# Combine system description and user prompt for models that don't support system roles\n",
        "combined_prompt = f\"{role_system_description}\\n\\n{user_prompt}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0115d4bc",
      "metadata": {},
      "source": [
        "## Generating and Displaying Output\n",
        "\n",
        "In the final step:\n",
        "\n",
        "1. **Define Search Criteria**: We specify a job search phrase and location.\n",
        "2. **Sample Candidate Data**: A random sample of job titles from the loaded data is selected.\n",
        "3. **Prepare Prompt Messages**: Messages are structured for the model:\n",
        "   - A system message sets the context for the AI.\n",
        "   - A user message defines the task: searching for suitable candidates based on criteria, with sample data included.\n",
        "4. **Generate Text**: The model generates text based on the prompt.\n",
        "5. **Display Output**: The generated text is displayed in Markdown format, making it readable and formatted nicely.\n",
        "\n",
        "This flow demonstrates how to integrate data processing, model interaction, and result display, offering both technical depth and executive overview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "673ac347",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare messages in the format required by the model\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": role_system_description},\n",
        "    {\"role\": \"user\", \"content\": user_prompt},\n",
        "]\n",
        "\n",
        "# Generate text\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 5_000,\n",
        "    \"return_full_text\": False,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "output = pipe(combined_prompt, **generation_args)\n",
        "display(Markdown(f\"Generated Output:\", output[0]['generated_text']))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "a4_llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
