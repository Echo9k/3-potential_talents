{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "930690b0",
      "metadata": {},
      "source": [
        "## Setting Up the Environment\n",
        "\n",
        "We begin by importing necessary libraries:\n",
        "\n",
        "- **os, json, pathlib.Path**: For file system operations and reading configuration files.\n",
        "- **pandas**: To handle data in structured formats.\n",
        "- **transformers**: To load and work with a language model from Hugging Face.\n",
        "- **IPython.display**: To display output in Markdown format for readability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c83ab6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        ")\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e32acf55",
      "metadata": {},
      "source": [
        "\n",
        "This step ensures the environment has all the required tools and libraries loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a21edd9",
      "metadata": {},
      "source": [
        "## Configuration and Credentials\n",
        "\n",
        "Next, we define and set up key file paths such as the root directory, data directory, and configuration directory. The script reads Hugging Face credentials from a `credentials.json` file. This allows secure access to private model resources. We also check if the environment variable `HUGGINGFACE_TOKEN` exists to ensure authentication with Hugging Face is configured properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6495ffb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Paths\n",
        "paths = {\n",
        "    'root': Path.cwd().parent,\n",
        "    'data': Path.cwd().parent / \"data\",\n",
        "    \"config\": Path.cwd().parent / \"config\"\n",
        "}\n",
        "\n",
        "# Load Hugging Face credentials\n",
        "with open(paths[\"config\"] / 'credentials.json') as f:\n",
        "    credentials = json.load(f)\n",
        "\n",
        "if \"HUGGINGFACE_TOKEN\" in os.environ or \"HUGGINGFACE_TOKEN\" in credentials:\n",
        "    print(\"Environment variable HUGGINGFACE_TOKEN set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c3c333f",
      "metadata": {},
      "source": [
        "## Reading Instructions\n",
        "\n",
        "The script attempts to open and read an `instructions.txt` file from the configuration directory. It handles potential errors gracefully:\n",
        "- Notifying if the file doesn't exist.\n",
        "- Catching and reporting any other exceptions that occur during file reading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "205e415e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Define the file path\n",
        "file_path = paths[\"config\"] / \"instructions.txt\"\n",
        "\n",
        "try:\n",
        "    # Open the file and read its content\n",
        "    with open(file_path, 'r') as file:\n",
        "        instructions = file.read()\n",
        "        print(\"Instructions successfully read!\")\n",
        "        # print(instructions)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c348ddb9",
      "metadata": {},
      "source": [
        "## Loading Candidate Data\n",
        "\n",
        "This section attempts to load candidate data from a Parquet file. If reading the Parquet file fails (e.g., due to format issues or missing file), it falls back to loading a CSV version. The data contains job titles and a ranking index, which is used for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5077ad60",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Shortlisted candidates\n",
        "try:\n",
        "    data = pd.read_parquet(paths['data'] / \"processed/filtered.parquet\", columns=['job_title', 'rank']).set_index(\"rank\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load parquet file: {e}. Loading CSV instead.\")\n",
        "    data = pd.read_csv(paths['data'] / \"processed/filtered.csv\", index_col=['rank'], usecols=['job_title'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a265859c",
      "metadata": {},
      "source": [
        "## Initializing the Language Model\n",
        "\n",
        "Here, we initialize a language model pipeline for text generation:\n",
        "\n",
        "1. **Select a Model**: We use Microsoft's Phi-3 model, a compact variant suitable for inference.\n",
        "2. **Check Device**: The script checks if a GPU is available and uses it for faster computation; otherwise, it falls back to CPU.\n",
        "3. **Load Model and Tokenizer**: The model and its tokenizer are loaded, and a text generation pipeline is set up for ease of use.\n",
        "\n",
        "This setup allows us to generate human-like text based on specified prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86392428",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Initialize the model and tokenizer\n",
        "model_name = \"microsoft/Phi-3-small-128k-instruct\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
        "\n",
        "# Adjust model loading for GPU\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    # torch_dtype=torch.float16,  # Use half precision\n",
        "    trust_remote_code=True,\n",
        "    device_map=device,  # Use GPU if available\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set up the pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0115d4bc",
      "metadata": {},
      "source": [
        "## Generating and Displaying Output\n",
        "\n",
        "In the final step:\n",
        "\n",
        "1. **Define Search Criteria**: We specify a job search phrase and location.\n",
        "2. **Sample Candidate Data**: A random sample of job titles from the loaded data is selected.\n",
        "3. **Prepare Prompt Messages**: Messages are structured for the model:\n",
        "   - A system message sets the context for the AI.\n",
        "   - A user message defines the task: searching for suitable candidates based on criteria, with sample data included.\n",
        "4. **Generate Text**: The model generates text based on the prompt.\n",
        "5. **Display Output**: The generated text is displayed in Markdown format, making it readable and formatted nicely.\n",
        "\n",
        "This flow demonstrates how to integrate data processing, model interaction, and result display, offering both technical depth and executive overview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "673ac347",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate results\n",
        "list_similar_roles = 'aspiring human resources'\n",
        "location = \"New York\"\n",
        "\n",
        "search_criteria = \"\"\"**Criteria:**\n",
        "- **Motivation:** Demonstrated interest and passion for a career in Human Resources.\n",
        "- **Experience & Background:** Relevant experience, transferable skills, and educational background.\n",
        "- **Fit with Company Values:** Highly motivated individuals with fundamental HR knowledge are preferred over those with extensive credentials.\n",
        "\"\"\"\n",
        "\n",
        "response_format = \"\"\"**Response Format:**\n",
        "Provide a ranked table in Markdown format with the following columns:\n",
        "- **Rank:** Position from 1 (top) to 5.\n",
        "- **Candidate:** Description of the candidate.\n",
        "- **Experience & Background:** Summary of relevant experience and qualifications.\n",
        "- **Reasoning:** Explanation for the ranking based on the criteria.\n",
        "\"\"\"\n",
        "\n",
        "# Sample data\n",
        "list_of_candidates = data['job_title'].sample(5, random_state=42).to_list()\n",
        "\n",
        "user_prompt = f\"\"\"**Prompt:**\n",
        "Valuate the list of candidates based on the search criteria and location: \"{location}\" for role(s) the role of {list_similar_roles}.\n",
        "\n",
        "{search_criteria}\n",
        "\n",
        "Identify and rank the top 5 candidates who are most suitable for a position in human resources. Present your findings in a Markdown-formatted table that includes the **Candidate**, **Experience & Background**, and **Reasoning for Choosing This Candidate**.\n",
        "\n",
        "**List of Candidates:**\n",
        "{list_of_candidates}\n",
        "\"\"\"\n",
        "\n",
        "role_system_description = 'You are an HR Recruitment Specialist. Your role is to assist in identifying, evaluating, and ranking job candidates based on their qualifications, experience, and suitability for specific roles. You prioritize motivated individuals with fundamental knowledge and present information in clear, organized formats to support hiring decisions.'\n",
        "\n",
        "# Prepare messages in the format required by the model\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": role_system_description},\n",
        "    {\"role\": \"user\", \"content\": user_prompt},\n",
        "]\n",
        "\n",
        "# Generate text\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 5_000,\n",
        "    \"return_full_text\": False,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "output = pipe(messages, **generation_args)\n",
        "display(Markdown(\"Generated Output:\", output[0]['generated_text']))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "a4_llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
