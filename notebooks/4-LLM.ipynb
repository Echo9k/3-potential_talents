{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Guill\\Miniconda3\\envs\\a4_llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variable HUGGINGFACE_TOKEN set.\n",
      "Instructions successfully read!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "import torch\n",
    "import aisuite as ai\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# Paths\n",
    "paths = {\n",
    "    'root': Path.cwd().parent,\n",
    "    'data': Path.cwd().parent / \"data\",\n",
    "    \"config\": Path.cwd().parent / \"config\"\n",
    "}\n",
    "\n",
    "# Load Hugging Face credentials\n",
    "with open(paths[\"config\"] / 'credentials.json') as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "\n",
    "if \"HUGGINGFACE_TOKEN\" in os.environ or \"HUGGINGFACE_TOKEN\" in credentials:\n",
    "    print(\"Environment variable HUGGINGFACE_TOKEN set.\")\n",
    "\n",
    "\n",
    "# Define the file path\n",
    "file_path = paths[\"config\"] / \"instructions.txt\"\n",
    "\n",
    "try:\n",
    "    # Open the file and read its content\n",
    "    with open(file_path, 'r') as file:\n",
    "        instructions = file.read()\n",
    "        print(\"Instructions successfully read!\")\n",
    "        # print(instructions)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Shortlisted candidates\n",
    "try:\n",
    "    list_of_candidates = pd.read_parquet(paths['data'] / \"processed/filtered.parquet\", columns=['job_title', 'rank']).set_index(\"rank\")['job_title'].to_list()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load parquet file: {e}. Loading CSV instead.\")\n",
    "    list_of_candidates = pd.read_csv(paths['data'] / \"processed/filtered.csv\", index_col=['rank'], usecols=['job_title'])['job_title'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.18s/it]\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and tokenizer\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
    "\n",
    "# Adjust model loading for GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=device,  # Use GPU if available\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set up the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "list_similar_roles = 'aspiring human resources'\n",
    "location = \"New York\"\n",
    "\n",
    "search_criteria = \"\"\"**Criteria:**\n",
    "- **Motivation:** Demonstrated interest and passion for a career in Human Resources.\n",
    "- **Experience & Background:** Relevant experience, transferable skills, and educational background.\n",
    "- **Fit with Company Values:** Highly motivated individuals with fundamental HR knowledge are preferred over those with extensive credentials.\n",
    "\"\"\"\n",
    "\n",
    "response_format = \"\"\"**Response Format:**\n",
    "Provide a ranked table in Markdown format with the following columns:\n",
    "- **Rank:** Position from 1 (top) to 5.\n",
    "- **Candidate:** Description of the candidate.\n",
    "- **Experience & Background:** Summary of relevant experience and qualifications.\n",
    "- **Reasoning:** Explanation for the ranking based on the criteria.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"**Prompt:**\n",
    "Valuate the list of candidates based on the search criteria and location: \"{location}\" for the role of {list_similar_roles}.\n",
    "\n",
    "{search_criteria}\n",
    "\n",
    "Identify and rank the top 5 candidates who are most suitable for a position in human resources. Present your findings in a Markdown-formatted table that includes the **Candidate**, **Experience & Background**, and **Reasoning for Choosing This Candidate**.\n",
    "\n",
    "**List of Candidates:**\n",
    "{list_of_candidates}\n",
    "\"\"\"\n",
    "\n",
    "role_system_description = (\n",
    "    \"You are an HR Recruitment Specialist. Your role is to assist in identifying, evaluating, \"\n",
    "    \"and ranking job candidates based on their qualifications, experience, and suitability for specific roles. \"\n",
    "    \"You prioritize motivated individuals with fundamental knowledge and present information in clear, organized formats to support hiring decisions.\"\n",
    ")\n",
    "\n",
    "\n",
    "# Prepare messages in the format required by the model\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": role_system_description},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "# Generate text\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 5_000,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "display(Markdown(\"Generated Output:\", output[0]['generated_text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a4_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
