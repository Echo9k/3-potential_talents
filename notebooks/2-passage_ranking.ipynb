{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24a272a1",
      "metadata": {},
      "source": [
        "# Passage Ranking\n",
        "\n",
        "This notebook evaluates and ranks candidates' fit for specific roles based on similarity scores. By leveraging machine learning and automation, we aim to streamline the recruitment process, reducing manual efforts and enhancing decision-making quality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a92f754",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "This step focuses on testing and optimizing the performance of our candidate ranking system. We:\n",
        "\n",
        "1. Set up the environment and load the necessary data and libraries.\n",
        "2. Compute similarity scores between job titles and predefined phrases.\n",
        "3. Filter, rank, and aggregate results to identify the best matches.\n",
        "4. Store the results for further analysis and integration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228ec44d",
      "metadata": {},
      "source": [
        "## Set up\n",
        "\n",
        "### Loading Libraries and Setting Paths\n",
        "\n",
        "We initialize the working environment, import libraries, and set up paths for data and configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8ef6727d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'g:\\\\Mi unidad\\\\wdir\\\\repos\\\\Apziva\\\\3-potential_talents'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    root_dir = \"/content/drive/MyDrive/wdir/repos/Apziva/3-potential_talents/\"\n",
        "    os.getcwd()\n",
        "\n",
        "except ImportError:\n",
        "    while 'potential_talents' not in os.listdir('.'):\n",
        "        os.chdir('..')\n",
        "        root_dir=os.getcwd()\n",
        "    \n",
        "    # append term_deposit to system to import custom functions\n",
        "    sys.path.append('.')\n",
        "    \n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac8e422",
      "metadata": {},
      "source": [
        "### Loading Data and API Setup\n",
        "\n",
        "Here, we load the encoded job titles and configure the API credentials for similarity computations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ff7fc532",
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'g:\\\\Mi unidad\\\\wdir\\\\repos\\\\Apziva\\\\3-potential_talents\\\\config\\\\.credentials'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(data_path  \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterim\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoded.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_title\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m credentials_path \u001b[38;5;241m=\u001b[39m Path(root_dir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.credentials\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m credentials \u001b[38;5;241m=\u001b[39m \u001b[43mtoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcredentials_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Define multiple search phrases for comparison\u001b[39;00m\n\u001b[0;32m     16\u001b[0m phrases_path \u001b[38;5;241m=\u001b[39m Path(root_dir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_phrases.toml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Guill\\Miniconda3\\envs\\talent\\lib\\site-packages\\toml\\decoder.py:133\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, _dict, decoder)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parses named file or files as toml and returns a dictionary\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    (Python 2 / Python 3)          file paths is passed\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _ispath(f):\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_getpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m ffile:\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loads(ffile\u001b[38;5;241m.\u001b[39mread(), _dict, decoder)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mlist\u001b[39m):\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'g:\\\\Mi unidad\\\\wdir\\\\repos\\\\Apziva\\\\3-potential_talents\\\\config\\\\.credentials'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import toml\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "data_path = Path(\"data\")\n",
        "data = pd.read_parquet(data_path  / \"interim\" / \"encoded.parquet\", columns=['job_title'])\n",
        "\n",
        "credentials_path = Path(root_dir) / \"config\" / \".credentials\"\n",
        "credentials = toml.load(credentials_path)\n",
        "\n",
        "# Define multiple search phrases for comparison\n",
        "phrases_path = Path(root_dir) / \"config\" / \"search_phrases.toml\"\n",
        "phrases = toml.load(phrases_path)['search_phrases']\n",
        "\n",
        "# API and credentials setup\n",
        "API_URL = \"https://api-inference.huggingface.co/models/sentence-transformers/msmarco-distilbert-base-tas-b\"\n",
        "headers = {\"Authorization\": f\"Bearer {credentials['hf_api_token']}\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a80d745",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Core Functions\n",
        "\n",
        "### Query Function\n",
        "\n",
        "The `query` function sends POST requests to the Hugging Face inference API to compute similarity scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31226719",
      "metadata": {},
      "outputs": [],
      "source": [
        "def query(payload):\n",
        "    \"\"\"Send a POST request to Hugging Face inference API.\"\"\"\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()  # Assuming the API returns a JSON response\n",
        "    else:\n",
        "        raise Exception(f\"API Error: {response.status_code} {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a1598a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Computing Similarities\n",
        "\n",
        "This function computes the similarity scores between multiple predefined phrases and job titles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1ca1f80",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_similarities(data, phrases):\n",
        "    \"\"\"Compute similarities between multiple phrases and job titles.\"\"\"\n",
        "    similarity_matrix = []\n",
        "    \n",
        "    for phrase in phrases:\n",
        "        payload = {\n",
        "            \"inputs\": {\n",
        "                \"source_sentence\": phrase,\n",
        "                \"sentences\": data['job_title'].tolist()\n",
        "            }\n",
        "        }\n",
        "        response = query(payload)\n",
        "        \n",
        "        if isinstance(response, dict) and 'similarities' in response:\n",
        "            scores = response['similarities']\n",
        "        elif isinstance(response, list):\n",
        "            scores = response\n",
        "        else:\n",
        "            raise TypeError(f\"Unexpected response format: {response}\")\n",
        "        \n",
        "        similarity_matrix.append(scores)\n",
        "    \n",
        "    return np.array(similarity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10a1605f",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Retry Mechanism for Robustness\n",
        "\n",
        "Added a retry mechanism to handle API timeouts or delays.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "960e9bb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_with_retry(payload, retries=5, delay=20):\n",
        "    \"\"\"Send a POST request to Hugging Face inference API with retry mechanism.\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        response = requests.post(API_URL, headers=headers, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        elif response.status_code == 503:\n",
        "            print(f\"Model is loading, retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "        else:\n",
        "            raise Exception(f\"API Error: {response.status_code} {response.text}\")\n",
        "    raise Exception(\"Max retries exceeded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e29bb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_similarities_with_retry(data, phrases):\n",
        "    \"\"\"Compute similarities between multiple phrases and job titles with retry mechanism.\"\"\"\n",
        "    similarity_matrix = []\n",
        "    \n",
        "    for phrase in phrases:\n",
        "        payload = {\n",
        "            \"inputs\": {\n",
        "                \"source_sentence\": phrase,\n",
        "                \"sentences\": data['job_title'].tolist()\n",
        "            }\n",
        "        }\n",
        "        response = query_with_retry(payload)\n",
        "        \n",
        "        # Debug the response structure\n",
        "        if isinstance(response, dict) and 'similarities' in response:\n",
        "            scores = response['similarities']\n",
        "        elif isinstance(response, list):  # Sometimes APIs return a list of scores\n",
        "            scores = response\n",
        "        else:\n",
        "            raise TypeError(f\"Unexpected response format: {response}\")\n",
        "        \n",
        "        similarity_matrix.append(scores)\n",
        "    \n",
        "    return np.array(similarity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c23fbb1",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Adding Similarity Scores to the DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a1db522",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute similarity scores\n",
        "similarity_matrix = compute_similarities_with_retry(data, phrases)\n",
        "\n",
        "# Add scores for each phrase to the DataFrame\n",
        "for i, phrase in enumerate(phrases):\n",
        "    data[f\"similarity_to_{phrase}\"] = similarity_matrix[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb97d129",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Filtering and Ranking\n",
        "\n",
        "This section filters and ranks job titles based on their similarity scores to each phrase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c97b0ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter and rank results for each phrase\n",
        "filtered_results = []\n",
        "for phrase in phrases:\n",
        "    filtered = (\n",
        "        data\n",
        "        .sort_values(f\"similarity_to_{phrase}\", ascending=False)\n",
        "    )\n",
        "    filtered['matching_phrase'] = phrase\n",
        "    filtered_results.append(filtered)\n",
        "\n",
        "# Combine filtered results into a single DataFrame\n",
        "final_result = pd.concat(filtered_results).drop_duplicates().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75c8e635",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Aggregation and Final Rankings\n",
        "\n",
        "We compute an aggregate fit score to rank the job titles effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f71a3bcb",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_result = final_result.assign(\n",
        "    fit=final_result.iloc[:, 1:-1].median(axis=1)-final_result.iloc[:, 1:-1].std(axis=1)\n",
        "    ).sort_values('fit', ascending=False).iloc[:, [0,-2,-1]]\n",
        "final_result.sample(4, random_state=27)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1511f21",
      "metadata": {},
      "source": [
        "Group and save the results for downstream analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a28373",
      "metadata": {},
      "outputs": [],
      "source": [
        "grouped_results = pd.DataFrame(\n",
        "    final_result.groupby('job_title')[\"fit\"].mean()\\\n",
        "    .sort_values(ascending=False)\n",
        "    )\n",
        "grouped_results.to_parquet(data_path / \"processed\" / \"grouped_results.parquet\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "talent",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
